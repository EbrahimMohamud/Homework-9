{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a5e378",
   "metadata": {},
   "source": [
    "## Week 11 Homework (9)\n",
    "\n",
    "### Q1: copulas\n",
    "\n",
    "1. ~~Use the example copula code below to provide posterior inference on the dependency structure between for **your own non normally distributed data that you find**~~\n",
    "2. Repeat the exercise using instead a two pass approach in the manner of https://www.pymc.io/projects/examples/en/latest/howto/copula-estimation.html\n",
    "3. Describe what a copula is and how the two verions of code implement it \n",
    "4. Describe how to use this to create arbitrary multivariate GLM regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c560370",
   "metadata": {},
   "source": [
    "3. A copula is a method for allowing you to express the joint multivariate distribution of univariate marginals by using a multivariate distribution like the Multivariate normal and specifiying the dependency structure between the random variables. Specifically, the marginal distributions are transformed using their (CDFs) to the standard normal scale via the inverse normal CDF from which we learn the dependency structure. \n",
    "\n",
    "> In the code, both versions implement this. They start by defining the marginal distributions for the data. Then they transform each observation into a uniform scale using the CDF's and then they associated standard normal value. They do this using the inverse normal CDF. The primary difference is in the coding style and organization. \n",
    "\n",
    "4. Copulas involve building a separate regression model for each outcome and then putting them all together. Using the inverse cdf of each outcome we turn the $X\\beta$ into ranging over 0 to 1. This is all then converted into a standard normal scale which we then combine all the outcomes together with to learn their dependency structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad602f",
   "metadata": {},
   "source": [
    "### Q2: Variable Selection using Spike and Slab\n",
    "\n",
    "Perform multivarite regression (or multivariate probit classification) with spike and slab variable selection priors and compare inference to analagous inference with diffuse normal priors (imposing minimal L2 style regularization on the likelihood).\n",
    "\n",
    "You may artificially limit the size of your data to reduce the computational demands, but if you do so, discuss the behavior of the computational demands with respect to the number of observations $n$, the number of random variables $m$ making up the multivariate observations, and the number of columns of the design matrix $p$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a844ed99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/multipledispatch/dispatcher.py:27: AmbiguityWarning: \n",
      "Ambiguities exist in dispatched function _unify\n",
      "\n",
      "The following signatures may result in ambiguous behavior:\n",
      "\t[object, ConstrainedVar, Mapping], [ConstrainedVar, Var, Mapping]\n",
      "\t[object, ConstrainedVar, Mapping], [ConstrainedVar, object, Mapping]\n",
      "\t[ConstrainedVar, object, Mapping], [object, ConstrainedVar, Mapping]\n",
      "\t[ConstrainedVar, Var, Mapping], [object, ConstrainedVar, Mapping]\n",
      "\n",
      "\n",
      "Consider making the following additions:\n",
      "\n",
      "@dispatch(ConstrainedVar, ConstrainedVar, Mapping)\n",
      "def _unify(...)\n",
      "\n",
      "@dispatch(ConstrainedVar, ConstrainedVar, Mapping)\n",
      "def _unify(...)\n",
      "\n",
      "@dispatch(ConstrainedVar, ConstrainedVar, Mapping)\n",
      "def _unify(...)\n",
      "\n",
      "@dispatch(ConstrainedVar, ConstrainedVar, Mapping)\n",
      "def _unify(...)\n",
      "  warn(warning_text(dispatcher.name, ambiguities), AmbiguityWarning)\n",
      "Multiprocess sampling (2 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">NUTS: [p_slab, beta_spike, beta_slab, packed_L]\n",
      ">BinaryGibbsMetropolis: [z]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2343' class='' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      58.58% [2343/4000 19:20&lt;13:41 Sampling 2 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import aesara.tensor as at\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. Load and Prepare Data\n",
    "# -----------------------------------------------------------\n",
    "df = pd.read_csv(\"Life Expectancy Data.csv\")\n",
    "# Drop missing rows in columns of interest\n",
    "df = df.dropna(subset=[\n",
    "    \"percentage expenditure\", \" BMI \", \"Schooling\", \"Population\",\n",
    "    \"Life expectancy \", \"Adult Mortality\", \"infant deaths\"\n",
    "])\n",
    "\n",
    "# Define predictor and outcome columns\n",
    "predictors = [\"percentage expenditure\", \" BMI \", \"Population\"]\n",
    "outcomes   = [\"Life expectancy \", \"infant deaths\"]\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_full = df[predictors].values\n",
    "y_full = df[outcomes].values\n",
    "\n",
    "# For demonstration, take a smaller subset\n",
    "n = 50\n",
    "df_trunc = df.sample(n=n, random_state=42)\n",
    "X_data = df_trunc[predictors].values\n",
    "y_data = df_trunc[outcomes].values\n",
    "\n",
    "# Dimensions\n",
    "n_rows, p = X_data.shape      # number of observations, number of predictors\n",
    "m = y_data.shape[1]           # number of outcome dimensions\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. Define the Spike-and-Slab Model\n",
    "# -----------------------------------------------------------\n",
    "with pm.Model() as spike_slab_model:\n",
    "    # (A) Probability that each beta_{j,k} is in the slab vs. spike\n",
    "    p_slab = pm.Beta(\"p_slab\", alpha=2, beta=2)\n",
    "    \n",
    "    # (B) Discrete indicators z_{j,k}: 0 => spike, 1 => slab\n",
    "    z = pm.Bernoulli(\"z\", p_slab, shape=(p, m))\n",
    "    \n",
    "    # (C) Spike and Slab distributions\n",
    "    #     For simplicity, fix their standard deviations.\n",
    "    sigma_spike = 0.001\n",
    "    sigma_slab  = 1.0\n",
    "    \n",
    "    # For each (j,k), we *could* do a mixture distribution, but an easier approach:\n",
    "    # define two sets of latent coefficients, then pick between them using z.\n",
    "    beta_spike = pm.Normal(\n",
    "        \"beta_spike\",\n",
    "        mu=0.0,\n",
    "        sigma=sigma_spike,\n",
    "        shape=(p, m)\n",
    "    )\n",
    "    beta_slab = pm.Normal(\n",
    "        \"beta_slab\",\n",
    "        mu=0.0,\n",
    "        sigma=sigma_slab,\n",
    "        shape=(p, m)\n",
    "    )\n",
    "    \n",
    "    # (D) Combine them in a deterministic way:\n",
    "    #     beta_{j,k} = z_{j,k}*beta_slab_{j,k} + (1 - z_{j,k})*beta_spike_{j,k}\n",
    "    betas = pm.Deterministic(\"betas\", z * beta_slab + (1 - z) * beta_spike)\n",
    "    \n",
    "    # (E) LKJ Cholesky prior for covariance among outcome dimensions\n",
    "    packed_L = pm.LKJCholeskyCov(\n",
    "        \"packed_L\",\n",
    "        n=m,\n",
    "        eta=2.0,\n",
    "        sd_dist=pm.Exponential.dist(1.0, shape=m),\n",
    "        compute_corr=False\n",
    "    )\n",
    "    L = pm.expand_packed_triangular(m, packed_L)\n",
    "    \n",
    "    # (F) Mean structure: mu_{i,k} = sum_j X[i,j]*betas_{j,k}\n",
    "    mu = pm.math.dot(X_data, betas)\n",
    "    \n",
    "    # (G) Multivariate Normal likelihood\n",
    "    y_obs = pm.MvNormal(\n",
    "        \"y_obs\",\n",
    "        mu=mu,\n",
    "        chol=L,\n",
    "        observed=y_data\n",
    "    )\n",
    "    \n",
    "    # (H) Sampling:\n",
    "    #     We typically sample continuous parameters with NUTS/HMC,\n",
    "    #     and discrete indicators with a discrete step method (e.g. Metropolis).\n",
    "    step = [\n",
    "        pm.NUTS(vars=[beta_spike, beta_slab, packed_L]),  # continuous parameters\n",
    "        pm.BinaryMetropolis(vars=[z])                     # discrete indicators\n",
    "    ]\n",
    "    \n",
    "    idata_ss = pm.sample(chains=2, random_seed=42)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. Summaries and Diagnostics\n",
    "# -----------------------------------------------------------\n",
    "print(az.summary(idata_ss, var_names=[\"betas\", \"z\", \"p_slab\"]))\n",
    "az.plot_trace(idata_ss, var_names=[\"betas\", \"z\", \"p_slab\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74eb166",
   "metadata": {},
   "source": [
    "### Q3 Variable Selection\n",
    "\n",
    "Perform multivarite regression (or multivariate probit classification) with the horseshoe variable selection prior and compare inference to analagous inference with spike and slab priors.\n",
    "\n",
    "The horseshoe variable selection prior is introduced here\n",
    "- https://www.pymc.io/projects/docs/en/v5.6.0/learn/core_notebooks/pymc_overview.html\n",
    "- and searches for \"horseshoe prior pymc\" on google produce additional examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77440e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. Load and Prepare Data\n",
    "# -----------------------------------------------------------\n",
    "df = pd.read_csv(\"Life Expectancy Data.csv\")\n",
    "# Drop missing rows in columns of interest\n",
    "df = df.dropna(subset=[\n",
    "    \"percentage expenditure\", \" BMI \", \"Schooling\", \"Population\",\n",
    "    \"Life expectancy \", \"Adult Mortality\", \"infant deaths\"\n",
    "])\n",
    "\n",
    "# Define predictor and outcome columns\n",
    "predictors = [\"percentage expenditure\", \" BMI \", \"Population\"]\n",
    "outcomes   = [\"Life expectancy \", \"infant deaths\"]\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_full = df[predictors].values\n",
    "y_full = df[outcomes].values\n",
    "\n",
    "# The full dataset size\n",
    "n_full = X_full.shape[0]\n",
    "\n",
    "# For demonstration, we use a smaller sample\n",
    "n = 50\n",
    "df_trunc = df.sample(n=n, random_state=42)\n",
    "X_data = df_trunc[predictors].values\n",
    "y_data = df_trunc[outcomes].values\n",
    "\n",
    "# Get shapes\n",
    "n_rows, p = X_data.shape\n",
    "m = y_data.shape[1]  # number of outcome dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2. Define and Fit the Model with Horseshoe Priors\n",
    "# -----------------------------------------------------------\n",
    "with pm.Model() as multivariate_model:\n",
    "    # 1) LKJ Cholesky prior for covariance among outcome dimensions\n",
    "    packed_L = pm.LKJCholeskyCov(\n",
    "        \"packed_L\",\n",
    "        n=m,\n",
    "        eta=2.0,\n",
    "        sd_dist=pm.Exponential.dist(1.0, shape=m),\n",
    "        compute_corr=False\n",
    "    )\n",
    "    L = pm.expand_packed_triangular(m, packed_L)\n",
    "\n",
    "    # 2) Horseshoe prior for regression coefficients\n",
    "    #    Global scale parameter tau ~ HalfCauchy\n",
    "    tau = pm.HalfCauchy(\"tau\", beta=1.0)\n",
    "\n",
    "    #    Local scale parameters lambdas_j ~ HalfCauchy\n",
    "    lambdas = pm.HalfCauchy(\"lambdas\", beta=1.0, shape=p)\n",
    "\n",
    "    #    The actual prior standard deviation for each beta_{j,k}\n",
    "    #    We broadcast tau*lambdas_j to each outcome dimension k\n",
    "    beta_sigma = tau * lambdas[:, None]  # shape = (p, m)\n",
    "\n",
    "    #    Finally, betas_{j,k} ~ Normal(0, sigma = tau * lambdas_j)\n",
    "    betas = pm.Normal(\"betas\", mu=0, sigma=beta_sigma, shape=(p, m))\n",
    "\n",
    "    # 3) Mean structure\n",
    "    mu = pm.math.dot(X_data, betas)\n",
    "\n",
    "    # 4) Multivariate normal likelihood\n",
    "    y_obs = pm.MvNormal(\"y_obs\", mu=mu, chol=L, observed=y_data)\n",
    "\n",
    "    # 5) Sample from the posterior\n",
    "    idata_hs = pm.sample(chains=2, random_seed=42)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. Summaries and Trace Plots\n",
    "# -----------------------------------------------------------\n",
    "summary_betas = az.summary(idata_hs, var_names=[\"betas\", \"tau\", \"lambdas\"])\n",
    "print(summary_betas)\n",
    "\n",
    "az.plot_trace(idata_hs, var_names=[\"betas\", \"tau\", \"lambdas\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
